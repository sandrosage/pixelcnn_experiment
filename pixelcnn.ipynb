{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastmri.data.subsample import create_mask_for_mask_type\n",
    "\n",
    "mask_type = \"random\" # or equispaced\n",
    "center_fractions = [0.08, 0.04]\n",
    "accelerations = [4, 8]\n",
    "mask_func = create_mask_for_mask_type(mask_type, center_fractions, accelerations)\n",
    "from modules.kspace_data import KspaceSample\n",
    "\n",
    "s = KspaceSample(0,1)\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pl_modules import PixelCNNModule, ReconstructKspaceDataModule\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "from modules.kspace_data import KspaceDataTransform\n",
    "\n",
    "model = PixelCNNModule()\n",
    "dm = ReconstructKspaceDataModule(\n",
    "    Path(\"knee_dataset\"),\n",
    "    challenge=\"singlecoil\",\n",
    "    train_transform=KspaceDataTransform(),\n",
    "    val_transform=KspaceDataTransform(),\n",
    "    test_transform=KspaceDataTransform(),\n",
    "    batch_size=1,\n",
    "    num_workers=4)\n",
    "trainer = pl.Trainer(max_epochs=2, default_root_dir=\"./pixelcnn/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | PixelCNN | 1.4 M  | train\n",
      "-------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.637     Total estimated model params size (MB)\n",
      "37        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:03<00:03,  0.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 2/7135 [00:27<27:24:55,  0.07it/s, v_num=9]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "from modules.model import PixelCNN, rearrange_kspace, laplace_nll\n",
    "from torch import optim\n",
    "model = PixelCNN()\n",
    "\n",
    "dl = dm.train_dataloader()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model.train(True)\n",
    "for i,batch in enumerate(dl):\n",
    "    print(i)\n",
    "    input = rearrange_kspace(batch.masked_kspace,0)\n",
    "    target = rearrange_kspace(batch.kspace,0)\n",
    "    mean, log = model(input)\n",
    "    loss = laplace_nll(mean,log,target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda to discuss\n",
    "- different kspace shapes (for forward diffusion equal shape is required)\n",
    "- output values (images: 256, because in paper they use softmax for probabilities)\n",
    "- split real and imaginary channel?\n",
    "- full kspace to full kspace, noisy kspace to full kspace, undersampled kspace to full kspace \n",
    "- different masking, 1D convolutions, multiplications instead of convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "(71293440,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "samples = None\n",
    "for i, sample in enumerate(dataloader):\n",
    "    point = sample.masked_kspace[:,:,:,0].numpy().ravel()\n",
    "    if i>0:\n",
    "        samples = np.append(samples, point)\n",
    "    else:\n",
    "        samples = point\n",
    "    if i >= 300:\n",
    "        print(i)\n",
    "        break\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['singlecoil_test', 'singlecoil_val']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"knee_singlecoil\")):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10873542\n"
     ]
    }
   ],
   "source": [
    "print(len(set(samples.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Step 1: Generate or load samples (example: normal distribution)\n",
    "# samples = np.random.normal(loc=0, scale=1, size=(1000,))  # 1000 samples from N(0, 1)\n",
    "# print(samples.shape)\n",
    "\n",
    "# Step 2a: Estimate PDF using a histogram\n",
    "# plt.hist([samples[:235520], samples[235520:2*235520]], bins=256, density=False, color=['g','b'], label=['Histogram1', 'Histogram2'], log=True, stacked=True)\n",
    "plt.hist(samples, bins=256, density=False, color='g', label='Histogram1', log=True)\n",
    "\n",
    "# Step 2b: Estimate PDF using Kernel Density Estimation (KDE)\n",
    "# kde = gaussian_kde(samples)\n",
    "# x = np.linspace(min(samples), max(samples), samples.shape[0])  # Create a range for x values# pdf = kde(x)  # Evaluate the KDE at these points\n",
    "# plt.plot(x, pdf, color='blue', label='KDE')\n",
    "\n",
    "# Step 3: Customize and display the plot\n",
    "plt.title('PDF of Samples')\n",
    "plt.xlabel('Sample Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  tensor(0.0618)\n",
      "Min:  tensor(-0.0298)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "from fastmri.data import transforms as T\n",
    "mi = 0\n",
    "ma = 0\n",
    "path = \"knee_singlecoil_val/singlecoil_val\"\n",
    "for file in os.listdir(path):\n",
    "    hf = h5py.File(os.path.join(path,file))\n",
    "    kspace =T.to_tensor(hf[\"kspace\"][()])\n",
    "    if kspace.min() < mi:\n",
    "        mi = kspace.min()\n",
    "    if kspace.max() > ma:\n",
    "        ma = kspace.max()\n",
    "print(\"Max: \", ma)\n",
    "print(\"Min: \", mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0298)\n",
      "tensor(0.0618)\n"
     ]
    }
   ],
   "source": [
    "# just for degbuggin purpose:\n",
    "mins = set()\n",
    "maxs = set()\n",
    "shapes = set()\n",
    "for batch in dataloader:\n",
    "    shapes.add(batch.masked_kspace.shape)\n",
    "    mins.add(batch.masked_kspace.min())\n",
    "    maxs.add(batch.masked_kspace.max())\n",
    "\n",
    "print(min(mins))\n",
    "print(max(maxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{torch.Size([1, 640, 320, 2]),\n",
       " torch.Size([1, 640, 338, 2]),\n",
       " torch.Size([1, 640, 356, 2]),\n",
       " torch.Size([1, 640, 368, 2]),\n",
       " torch.Size([1, 640, 372, 2]),\n",
       " torch.Size([1, 640, 386, 2]),\n",
       " torch.Size([1, 640, 400, 2]),\n",
       " torch.Size([1, 640, 454, 2]),\n",
       " torch.Size([1, 640, 644, 2])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.model import PixelCNN\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "model = PixelCNN(in_channels=1)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, dataloader, n_epochs, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.n_epochs = n_epochs\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer(self.model.parameters())\n",
    "    \n",
    "    def train(self):\n",
    "        model.train(True)\n",
    "        for epoch in tqdm(range(self.n_epochs), desc=\"Epochs\"):\n",
    "            epoch_loss = 0.0\n",
    "            with tqdm(self.dataloader, desc=f\"Epoch {epoch+1}\", unit=\"batch\") as tepoch:\n",
    "                for batch in tepoch:\n",
    "                    input = batch.masked_kspace.permute(0,3,1,2)[:,0,:].unsqueeze(1)\n",
    "                    target = batch.target.permute(0,3,1,2)[:,0,:].long()\n",
    "                    output = self.model(input)\n",
    "                    loss = self.criterion(output, target)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    tepoch.set_postfix(loss=loss.item())\n",
    "                    epoch_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}/{self.n_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(PixelCNN(), dataloader, 5, CrossEntropyLoss(), optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   8%|▊         | 591/7135 [1:43:53<19:10:24, 10.55s/batch, loss=5.61]\n",
      "Epochs:   0%|          | 0/5 [1:43:53<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mmasked_kspace\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)[:,\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m target \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)[:,\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output, target)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\modules\\model.py:208\u001b[0m, in \u001b[0;36mPixelCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    206\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input(x)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers:\n\u001b[1;32m--> 208\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_head(x)\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\modules\\model.py:29\u001b[0m, in \u001b[0;36mMaskedConv2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMaskedConv2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/7135 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print(\"Target shape: \", target.shape)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# print(\"Output shape: \", model(input).shape)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# target = Variable((input.data[:,0] * 255).long())\u001b[39;00m\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(model(\u001b[38;5;28minput\u001b[39m), target)\n\u001b[1;32m---> 11\u001b[0m loss_train\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "loss_train = []\n",
    "for epoch in tqdm(range(5), desc=\"Epochs\"):\n",
    "    model.train(True)\n",
    "for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "    input = batch.masked_kspace.permute(0,3,1,2)[:,0,:].unsqueeze(1)\n",
    "    # print(\"Input shape: \",input.shape)\n",
    "    target = batch.target.permute(0,3,1,2)[:,0,:].long()\n",
    "    # print(\"Target shape: \", target.shape)\n",
    "    # print(\"Output shape: \", model(input).shape)\n",
    "    # target = Variable((input.data[:,0] * 255).long())\n",
    "    loss = F.cross_entropy(model(input), target)\n",
    "    loss_train.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 5.6579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 11.3201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 16.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 22.6377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 28.2965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 33.9540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 39.6133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 45.2711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 50.9296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 56.5891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 62.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 67.8991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 73.5585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 79.2170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 84.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 90.5316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 96.1931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 101.8493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 107.5085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 113.1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 118.8277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▍         | 21/469 [01:39<35:13,  4.72s/batch, loss=5.66]\n",
      "Epochs:   0%|          | 0/5 [01:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m                     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m mnist_trainer \u001b[38;5;241m=\u001b[39m TrainerMNIST(net, tr, \u001b[38;5;241m5\u001b[39m, CrossEntropyLoss(), optim\u001b[38;5;241m.\u001b[39mAdam)\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmnist_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 65\u001b[0m, in \u001b[0;36mTrainerMNIST.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output, target)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 65\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     67\u001b[0m tepoch\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim, cuda, backends\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms, utils\n",
    "backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        assert mask_type in {'A', 'B'}\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, kH, kW = self.weight.size()\n",
    "        self.mask.fill_(1)\n",
    "        self.mask[:, :, kH // 2, kW // 2 + (mask_type == 'B'):] = 0\n",
    "        self.mask[:, :, kH // 2 + 1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n",
    "\n",
    "fm = 64\n",
    "net = nn.Sequential(\n",
    "    MaskedConv2d('A', 1,  fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    nn.Conv2d(fm, 256, 1))\n",
    "\n",
    "tr = data.DataLoader(datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "                     batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "te = data.DataLoader(datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor()),\n",
    "                     batch_size=128, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "class TrainerMNIST():\n",
    "    def __init__(self, model, dataloader, n_epochs, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.n_epochs = n_epochs\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer(self.model.parameters())\n",
    "    \n",
    "    def train(self):\n",
    "        model.train(True)\n",
    "        for epoch in tqdm(range(self.n_epochs), desc=\"Epochs\"):\n",
    "            epoch_loss = 0.0\n",
    "            with tqdm(self.dataloader, desc=f\"Epoch {epoch+1}\", unit=\"batch\") as tepoch:\n",
    "                for input,_ in tepoch:\n",
    "                    input = Variable(input)\n",
    "                    target = Variable((input.data[:,0] * 255).long())\n",
    "                    output = self.model(input)\n",
    "                    loss = self.criterion(output, target)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    tepoch.set_postfix(loss=loss.item())\n",
    "                    epoch_loss += loss.item()\n",
    "                    print(f\"Epoch {epoch+1}/{self.n_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "mnist_trainer = TrainerMNIST(net, tr, 5, CrossEntropyLoss(), optim.Adam)\n",
    "mnist_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 640, 320])\n",
      "torch.Size([1, 2, 256, 640, 320])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input tensor of shape (1, 2, 640, 320)\n",
    "input_tensor = torch.randn(1, 2, 640, 320)\n",
    "\n",
    "# Define the grouped 2D convolution\n",
    "conv = nn.Conv2d(\n",
    "    in_channels=2,   # Input channels\n",
    "    out_channels=256 * 2,  # Output channels (256 for each input channel)\n",
    "    kernel_size=1,    # Kernel size 1x1 (no change to spatial dimensions)\n",
    "    groups=2          # Groups = in_channels (for separate convolutions per channel)\n",
    ")\n",
    "\n",
    "# Apply the convolution\n",
    "output = conv(input_tensor)\n",
    "\n",
    "print(output.shape)\n",
    "# Reshape the output to the desired shape (1, 2, 256, 640, 320)\n",
    "output = output.view(1, 2, 256, 640, 320)\n",
    "\n",
    "# Print the final shape\n",
    "print(output.shape)  # torch.Size([1, 2, 256, 640, 320])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 640, 320])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor with shape [1, 640, 320, 2]\n",
    "tensor = torch.randn(1, 640, 320, 2)\n",
    "\n",
    "# Rearrange dimensions to [1, 2, 640, 320]\n",
    "tensor_reshaped = tensor.permute(0, -1, 1, 2)\n",
    "tensor_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original values: [-0.0298  0.      0.03    0.0618]\n",
      "Normalized values: [0.         0.32532751 0.65283843 1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example values ranging from -0.0298 to 0.0618\n",
    "values = np.array([-0.0298, 0.0, 0.03, 0.0618])  # Replace with your actual data\n",
    "\n",
    "# Min and Max of the original range\n",
    "min_val = -0.0298\n",
    "max_val = 0.0618\n",
    "\n",
    "# Normalization to range [0, 1]\n",
    "normalized_values = (values - min_val) / (max_val - min_val)\n",
    "\n",
    "print(\"Original values:\", values)\n",
    "print(\"Normalized values:\", normalized_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gated_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m----> 2\u001b[0m summary(\u001b[43mgated_model\u001b[49m, (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gated_model' is not defined"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(gated_model, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms, utils\n",
    "tr = data.DataLoader(datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "                     batch_size=1, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128, 1, 28, 28])\n",
      "Mean:  torch.Size([128, 1, 28, 28])\n",
      "Log:  torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     48\u001b[0m time_tr \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time_tr\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sandr\\source\\repos\\pixelcnn_experiment\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.distributions import Laplace\n",
    "import numpy as np\n",
    "import time\n",
    "from modules.model import PixelCNN, laplace_nll\n",
    "\n",
    "net = PixelCNN()\n",
    "\n",
    "# Data Loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.0,), std=(1.0,))  # Assume Laplace distributed inputs are mean 0, std 1\n",
    "])\n",
    "\n",
    "train_loader = data.DataLoader(datasets.MNIST('data', train=True, download=True, transform=transform),\n",
    "                                batch_size=128, shuffle=True)\n",
    "test_loader = data.DataLoader(datasets.MNIST('data', train=False, transform=transform),\n",
    "                               batch_size=128, shuffle=False)\n",
    "\n",
    "# Training Setup\n",
    "sample = torch.zeros(144, 1, 28, 28)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(25):\n",
    "    # Training Phase\n",
    "    net.train()\n",
    "    train_loss = []\n",
    "    time_tr = time.time()\n",
    "    for input, _ in train_loader:\n",
    "        input = input\n",
    "        print(input.shape)\n",
    "        target = input\n",
    "\n",
    "        mean, log_scale = net(input)  # Predict mean and log_scale\n",
    "        print(\"Mean: \", mean.shape)\n",
    "        print(\"Log: \", log_scale.shape)\n",
    "        loss = laplace_nll(mean, log_scale, target)  # Laplace NLL loss\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    time_tr = time.time() - time_tr\n",
    "\n",
    "    # Testing Phase\n",
    "    net.eval()\n",
    "    test_loss = []\n",
    "    time_te = time.time()\n",
    "    with torch.no_grad():\n",
    "        for input, _ in test_loader:\n",
    "            input = input.cuda()\n",
    "            target = input\n",
    "\n",
    "            mean, log_scale = net(input)  # Predict mean and log_scale\n",
    "            loss = laplace_nll(mean, log_scale, target)  # Laplace NLL loss\n",
    "            test_loss.append(loss.item())\n",
    "    time_te = time.time() - time_te\n",
    "\n",
    "    # Sampling\n",
    "    sample.fill_(0)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                mean, log_scale = net(sample)\n",
    "                scale = torch.exp(log_scale[:, :, i, j])  # Convert log_scale to scale\n",
    "                dist = Laplace(mean[:, :, i, j], scale)\n",
    "                sample[:, :, i, j] = dist.sample().clamp(0, 1)  # Sample and clamp values to [0, 1]\n",
    "\n",
    "    utils.save_image(sample, 'sample_{:02d}.png'.format(epoch), nrow=12, padding=0)\n",
    "\n",
    "    print(f'Epoch {epoch}: Train Loss={np.mean(train_loss):.7f}, '\n",
    "          f'Test Loss={np.mean(test_loss):.7f}, '\n",
    "          f'Train Time={time_tr:.2f}s, Test Time={time_te:.2f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixelvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
